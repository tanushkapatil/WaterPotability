# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.naive_bayes import GaussianNB
import joblib


# Load the dataset
df = pd.read_csv("data/water_potability.csv")


# Initial exploration
print("Dataset shape:", df.shape)
print("\nMissing values:\n", df.isnull().sum())
print("\nData types:\n", df.dtypes)


# Visualize missing values
plt.figure(figsize=(10, 5))
df.isnull().mean().sort_values(ascending=False).plot.bar()
plt.title("Percentage of Missing Values by Feature")
plt.ylabel("Percentage")
plt.xlabel("Features")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# Handle missing values
for col in df.columns:
    if df[col].isnull().sum() > 0:
        if col in ['ph', 'Sulfate', 'Trihalomethanes']:
            df[col] = df[col].fillna(df[col].median())


# Feature engineering
df['TDS_to_Hardness'] = df['Solids'] / df['Hardness']
df['Chloramine_to_Trihalomethanes'] = df['Chloramines'] / df['Trihalomethanes']


# Visualize correlations
plt.figure(figsize=(12, 8))
corr = df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Feature Correlation Matrix")
plt.tight_layout()
plt.show()


# Store original for comparison
df_original = df.copy()

# Outlier removal function
def remove_outliers(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        df = df[~((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR)))]
    return df

# Identify numerical columns
numerical_cols = df.select_dtypes(include=['float64']).columns.tolist()

# Remove outliers
df_no_outliers = remove_outliers(df_original, numerical_cols)

# Plotting side-by-side boxplots
fig, axes = plt.subplots(1, 2, figsize=(20, 8), sharey=True)

# Before
sns.boxplot(data=df_original[numerical_cols], ax=axes[0])
axes[0].set_title("Before Outlier Removal")
axes[0].tick_params(axis='x', rotation=45)

# After
sns.boxplot(data=df_no_outliers[numerical_cols], ax=axes[1])
axes[1].set_title("After Outlier Removal")
axes[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()


print("Before outlier removal:", df_original.shape)
print("After outlier removal:", df_no_outliers.shape)



def remove_outliers_verbose(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        before = df.shape[0]
        df = df[(df[col] >= lower) & (df[col] <= upper)]
        after = df.shape[0]
        print(f"{col}: Removed {before - after} rows")
    return df

df_no_outliers = remove_outliers_verbose(df_original.copy(), numerical_cols)



col = 'Solids'  # Change to any column you're interested in
sns.boxplot(x=df_original[col])
plt.title(f'Boxplot of {col} Before Removal')
plt.show()

sns.boxplot(x=df_no_outliers[col])
plt.title(f'Boxplot of {col} After Removal')
plt.show()



import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Outlier removal function
def remove_outliers_columnwise(df, columns):
    df_cleaned = df.copy()
    for col in columns:
        Q1 = df_cleaned[col].quantile(0.25)
        Q3 = df_cleaned[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        before = df_cleaned.shape[0]
        df_cleaned = df_cleaned[(df_cleaned[col] >= lower) & (df_cleaned[col] <= upper)]
        after = df_cleaned.shape[0]
        print(f"{col}: Removed {before - after} rows")
    return df_cleaned

# Select numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Save original
df_original = df.copy()

# Remove outliers
df_no_outliers = remove_outliers_columnwise(df_original, numerical_cols)

# Plot comparison for a specific column (e.g., 'Solids')
col = 'Solids'

# Before
sns.boxplot(x=df_original[col])
plt.title(f'{col} Before Outlier Removal')
plt.show()

# After
sns.boxplot(x=df_no_outliers[col])
plt.title(f'{col} After Outlier Removal')
plt.show()


# Prepare data for modeling
X = df.drop('Potability', axis=1)
y = df['Potability']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


# Scale numerical features
scaler = StandardScaler()
cols_to_scale = ['Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 
                 'Organic_carbon', 'Trihalomethanes', 'Turbidity', 'TDS_to_Hardness',
                 'Chloramine_to_Trihalomethanes']

X_train[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])
X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])


# Train Random Forest with hyperparameter tuning
rf_params = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

rf = RandomForestClassifier(random_state=42)
rf_random = RandomizedSearchCV(estimator=rf, param_distributions=rf_params, 
                              n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)
rf_random.fit(X_train, y_train)


# Evaluate Random Forest
best_rf = rf_random.best_estimator_
y_pred_rf = best_rf.predict(X_test)
print("Random Forest Results:")
print("Best Parameters:", rf_random.best_params_)
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))


# Train Gaussian Naive Bayes
gnb = GaussianNB()
gnb.fit(X_train, y_train)


# Evaluate Naive Bayes
y_pred_gnb = gnb.predict(X_test)
print("\nNaive Bayes Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_gnb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_gnb))


# Save the best model and scaler
joblib.dump(gnb, "models/model.pkl")
joblib.dump(scaler, "models/scaler.pkl")


# Feature importance for Random Forest
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_rf.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance)
plt.title("Feature Importance (Random Forest)")
plt.tight_layout()
plt.show()



